{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing noise...\n",
      "Transcribing...\n",
      "Running diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhlab/miniconda3/envs/digimab/lib/python3.10/site-packages/pyannote/audio/models/blocks/pooling.py:103: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nessun metodo funziona: 'DiarizeOutput' object is not iterable\n",
      "Tipo oggetto: <class 'pyannote.audio.pipelines.speaker_diarization.DiarizeOutput'>\n",
      "Attributi: ['exclusive_speaker_diarization', 'serialize', 'speaker_diarization', 'speaker_embeddings']\n",
      "Segmenti diarization trovati: 0\n",
      "Segmenti dopo merge: 1\n",
      "Saving diarized transcription...\n",
      "Trascrizione diarizzata salvata: trascrizione_diarizzata.txt / .json\n",
      "Classifying content type...\n",
      "üìÇ Contenuto classificato come: 'convegno' (confidenza: 95%)\n",
      "   Motivazione: Il testo trascritto √® chiaramente una riunione/convegno istituzionale e accademico dedicato alle celebrazioni del bicentenario leopardiano. Sono presenti pi√π relatori (assessori, ambasciatori, rappresentanti di ministeri, professori universitari) che si alternano in interventi formali, si discutono programmi nazionali e internazionali, coordinamenti tra istituzioni, leggi di finanziamento e comitati scientifici. Il tono e la struttura corrispondono a un convegno/seminario istituzionale con pi√π partecipanti.\n",
      "Processing chunk 1/2\n",
      "Processing chunk 2/2\n",
      "\n",
      "‚úÖ Pipeline completa.\n",
      "   üìÑ trascrizione_diarizzata.txt  ‚Äî trascrizione con speaker e timestamp\n",
      "   üìã trascrizione_diarizzata.json ‚Äî trascrizione strutturata in JSON\n",
      "   üìù regesto_finale.txt           ‚Äî regesto accademico\n",
      "   üì¶ regesto_finale.json          ‚Äî regesto in JSON\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import json\n",
    "import math\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "from pyannote.audio import Pipeline\n",
    "from collections import defaultdict\n",
    "import anthropic\n",
    "import os\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "AUDIO_PATH = \"audio_input/001.mp3\"\n",
    "BASE_NAME = os.path.splitext(os.path.basename(AUDIO_PATH))[0] \n",
    "CLEAN_AUDIO_PATH = \"convegno_clean.wav\"\n",
    "HF_TOKEN = \n",
    "ANTHROPIC_API_KEY = \n",
    "CLAUDE_MODEL = \"claude-sonnet-4-6\"\n",
    "\n",
    "CHUNK_WORD_LIMIT = 4000\n",
    "\n",
    "# =========================\n",
    "# 1. NOISE REDUCTION\n",
    "# =========================\n",
    "\n",
    "print(\"Reducing noise...\")\n",
    "audio, sr = librosa.load(AUDIO_PATH, sr=None)\n",
    "noise_sample = audio[0:5*sr]\n",
    "\n",
    "reduced_noise = nr.reduce_noise(\n",
    "    y=audio,\n",
    "    sr=sr,\n",
    "    y_noise=noise_sample,\n",
    "    prop_decrease=0.8\n",
    ")\n",
    "\n",
    "sf.write(CLEAN_AUDIO_PATH, reduced_noise, sr)\n",
    "\n",
    "# =========================\n",
    "# 2. LOAD MODELS\n",
    "# =========================\n",
    "\n",
    "whisper_model = whisper.load_model(\"medium\")\n",
    "\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "# =========================\n",
    "# 3. TRANSCRIPTION\n",
    "# =========================\n",
    "\n",
    "print(\"Transcribing...\")\n",
    "transcription = whisper_model.transcribe(\n",
    "    CLEAN_AUDIO_PATH,\n",
    "    language=\"it\",\n",
    "    word_timestamps=True\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4. DIARIZATION\n",
    "# =========================\n",
    "\n",
    "print(\"Running diarization...\")\n",
    "diarization = diarization_pipeline(CLEAN_AUDIO_PATH)\n",
    "\n",
    "# =========================\n",
    "# 5. MERGE TRANSCRIPT + SPEAKERS\n",
    "# =========================\n",
    "\n",
    "segments_output = []\n",
    "\n",
    "diar_list = []\n",
    "\n",
    "try:\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        diar_list.append((turn.start, turn.end, speaker))\n",
    "    print(\"Metodo: itertracks\")\n",
    "except AttributeError:\n",
    "    try:\n",
    "        for row in diarization.segments:\n",
    "            diar_list.append((row.start, row.end, row.speaker))\n",
    "        print(\"Metodo: segments\")\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            for row in diarization:\n",
    "                diar_list.append((row.start, row.end, row.speaker))\n",
    "            print(\"Metodo: iterabile diretto\")\n",
    "        except Exception as e:\n",
    "            print(f\"Nessun metodo funziona: {e}\")\n",
    "            print(f\"Tipo oggetto: {type(diarization)}\")\n",
    "            print(f\"Attributi: {[x for x in dir(diarization) if not x.startswith('_')]}\")\n",
    "\n",
    "print(f\"Segmenti diarization trovati: {len(diar_list)}\")\n",
    "if diar_list:\n",
    "    print(f\"Esempio: {diar_list[0]}\")\n",
    "\n",
    "for segment in transcription[\"segments\"]:\n",
    "    seg_start = segment[\"start\"]\n",
    "    seg_end = segment[\"end\"]\n",
    "    seg_text = segment[\"text\"].strip()\n",
    "\n",
    "    speaker_label = \"UNKNOWN\"\n",
    "    best_overlap = 0\n",
    "\n",
    "    for (d_start, d_end, speaker) in diar_list:\n",
    "        overlap = min(d_end, seg_end) - max(d_start, seg_start)\n",
    "        if overlap > best_overlap:\n",
    "            best_overlap = overlap\n",
    "            speaker_label = speaker\n",
    "\n",
    "    segments_output.append({\n",
    "        \"speaker\": speaker_label,\n",
    "        \"start\": seg_start,\n",
    "        \"end\": seg_end,\n",
    "        \"text\": seg_text\n",
    "    })\n",
    "\n",
    "# Merge segmenti consecutivi dello stesso speaker\n",
    "merged_segments = []\n",
    "current = segments_output[0]\n",
    "\n",
    "for seg in segments_output[1:]:\n",
    "    if seg[\"speaker\"] == current[\"speaker\"]:\n",
    "        current[\"end\"] = seg[\"end\"]\n",
    "        current[\"text\"] += \" \" + seg[\"text\"]\n",
    "    else:\n",
    "        merged_segments.append(current)\n",
    "        current = seg\n",
    "\n",
    "merged_segments.append(current)\n",
    "\n",
    "print(f\"Segmenti dopo merge: {len(merged_segments)}\")\n",
    "\n",
    "# =========================\n",
    "# 6. IDENTIFY ROLES\n",
    "# =========================\n",
    "\n",
    "speaker_stats = defaultdict(lambda: {\n",
    "    \"total_duration\": 0,\n",
    "    \"num_segments\": 0,\n",
    "    \"total_words\": 0\n",
    "})\n",
    "\n",
    "for seg in merged_segments:\n",
    "    duration = seg[\"end\"] - seg[\"start\"]\n",
    "    words = len(seg[\"text\"].split())\n",
    "\n",
    "    speaker_stats[seg[\"speaker\"]][\"total_duration\"] += duration\n",
    "    speaker_stats[seg[\"speaker\"]][\"num_segments\"] += 1\n",
    "    speaker_stats[seg[\"speaker\"]][\"total_words\"] += words\n",
    "\n",
    "sorted_speakers = sorted(\n",
    "    speaker_stats.items(),\n",
    "    key=lambda x: x[1][\"total_duration\"],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "role_mapping = {}\n",
    "\n",
    "if sorted_speakers:\n",
    "    role_mapping[sorted_speakers[0][0]] = \"RELATORE_PRINCIPALE\"\n",
    "\n",
    "for speaker, stats in sorted_speakers[1:]:\n",
    "    avg_words = stats[\"total_words\"] / max(stats[\"num_segments\"], 1)\n",
    "\n",
    "    if stats[\"total_duration\"] > 0.15 * sorted_speakers[0][1][\"total_duration\"]:\n",
    "        role_mapping[speaker] = \"RELATORE_SECONDARIO\"\n",
    "    elif avg_words < 40:\n",
    "        role_mapping[speaker] = \"PUBBLICO\"\n",
    "    else:\n",
    "        role_mapping[speaker] = \"INTERVENTO\"\n",
    "\n",
    "for seg in merged_segments:\n",
    "    seg[\"role\"] = role_mapping.get(seg[\"speaker\"], \"UNKNOWN\")\n",
    "\n",
    "# =========================\n",
    "# 7. SAVE DIARIZED TRANSCRIPTION\n",
    "# =========================\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    \"\"\"Converte secondi in formato HH:MM:SS\"\"\"\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = int(seconds % 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "# Salva trascrizione diarizzata come TXT leggibile\n",
    "print(\"Saving diarized transcription...\")\n",
    "with open(f\"{BASE_NAME}trascrizione_diarizzata.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"TRASCRIZIONE DIARIZZATA\\n\")\n",
    "    f.write(f\"File: {AUDIO_PATH}\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    for seg in merged_segments:\n",
    "        ts_start = format_timestamp(seg[\"start\"])\n",
    "        ts_end = format_timestamp(seg[\"end\"])\n",
    "        f.write(f\"[{ts_start} ‚Üí {ts_end}]  {seg['role']} ({seg['speaker']})\\n\")\n",
    "        f.write(f\"{seg['text']}\\n\\n\")\n",
    "\n",
    "# Salva trascrizione diarizzata come JSON strutturato\n",
    "with open(f\"{BASE_NAME}trascrizione_diarizzata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"file_name\": AUDIO_PATH,\n",
    "        \"num_speakers\": len(speaker_stats),\n",
    "        \"speaker_stats\": {\n",
    "            spk: {\n",
    "                \"role\": role_mapping.get(spk, \"UNKNOWN\"),\n",
    "                \"total_duration_sec\": round(stats[\"total_duration\"], 2),\n",
    "                \"num_segments\": stats[\"num_segments\"],\n",
    "                \"total_words\": stats[\"total_words\"]\n",
    "            }\n",
    "            for spk, stats in speaker_stats.items()\n",
    "        },\n",
    "        \"segments\": [\n",
    "            {\n",
    "                \"speaker\": seg[\"speaker\"],\n",
    "                \"role\": seg[\"role\"],\n",
    "                \"start\": round(seg[\"start\"], 2),\n",
    "                \"end\": round(seg[\"end\"], 2),\n",
    "                \"duration\": round(seg[\"end\"] - seg[\"start\"], 2),\n",
    "                \"text\": seg[\"text\"]\n",
    "            }\n",
    "            for seg in merged_segments\n",
    "        ]\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Trascrizione diarizzata salvata: trascrizione_diarizzata.txt / .json\")\n",
    "\n",
    "# =========================\n",
    "# 8. BUILD FULL TEXT FOR REGESTO\n",
    "# =========================\n",
    "\n",
    "full_text = \"\"\n",
    "for seg in merged_segments:\n",
    "    full_text += f\"{seg['role']}: {seg['text']}\\n\\n\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8b. CONTENT CLASSIFICATION\n",
    "# =========================\n",
    "\n",
    "CONTENT_TYPES = {\n",
    "    \"convegno\": {\n",
    "        \"description\": \"Conferenza accademica, convegno, seminario, lezione universitaria\",\n",
    "        \"system_prompt\": \"Sei un redattore accademico esperto nella redazione di regesti scientifici.\",\n",
    "        \"chunk_prompt\": \"\"\"\n",
    "Redigi un regesto analitico della seguente parte di convegno accademico.\n",
    "Non distinguere per speaker. Usa linguaggio formale scientifico.\n",
    "Evidenzia: tesi principale, argomentazioni, riferimenti bibliografici, conclusioni parziali.\n",
    "\n",
    "{chunk}\n",
    "\"\"\",\n",
    "        \"final_prompt\": \"\"\"\n",
    "Integra i seguenti regesti parziali in un unico regesto coerente del convegno.\n",
    "Struttura: \n",
    "- Tema centrale e obiettivi\n",
    "- Argomentazioni principali sviluppate\n",
    "- Riferimenti e fonti citate\n",
    "- Conclusioni e prospettive di ricerca\n",
    "Evita ripetizioni. Nessun elenco puntato. Massimo 300 parole.\n",
    "\n",
    "{combined}\n",
    "\"\"\"\n",
    "    },\n",
    "    \"dibattito\": {\n",
    "        \"description\": \"Dibattito, intervista, talk show, confronto tra posizioni diverse\",\n",
    "        \"system_prompt\": \"Sei un analista esperto nella sintesi di dibattiti e confronti dialettici.\",\n",
    "        \"chunk_prompt\": \"\"\"\n",
    "Redigi una sintesi analitica di questa parte di dibattito/intervista.\n",
    "Identifica le posizioni espresse, senza attribuirle a singoli speaker.\n",
    "Metti in evidenza: punti di accordo, punti di contrasto, argomenti chiave.\n",
    "\n",
    "{chunk}\n",
    "\"\"\",\n",
    "        \"final_prompt\": \"\"\"\n",
    "Integra i seguenti estratti in un unico regesto coerente del dibattito.\n",
    "Struttura:\n",
    "- Tema e contesto del confronto\n",
    "- Posizioni principali emerse\n",
    "- Punti di tensione o disaccordo\n",
    "- Elementi di convergenza o sintesi finale\n",
    "Evita ripetizioni. Nessun elenco puntato. Massimo 300 parole.\n",
    "\n",
    "{combined}\n",
    "\"\"\"\n",
    "    },\n",
    "    \"discorso_politico\": {\n",
    "        \"description\": \"Discorso istituzionale, comizio, intervento politico, cerimonia ufficiale\",\n",
    "        \"system_prompt\": \"Sei un analista politico esperto nella sintesi di discorsi istituzionali.\",\n",
    "        \"chunk_prompt\": \"\"\"\n",
    "Redigi una sintesi di questa parte di discorso istituzionale/politico.\n",
    "Identifica: messaggi chiave, riferimenti a politiche o eventi, tono e registro comunicativo.\n",
    "\n",
    "{chunk}\n",
    "\"\"\",\n",
    "        \"final_prompt\": \"\"\"\n",
    "Integra i seguenti estratti in un unico regesto del discorso.\n",
    "Struttura:\n",
    "- Contesto e occasione\n",
    "- Messaggi e temi centrali\n",
    "- Riferimenti a fatti, dati o politiche\n",
    "- Appelli e conclusioni\n",
    "Evita ripetizioni. Nessun elenco puntato. Massimo 300 parole.\n",
    "\n",
    "{combined}\n",
    "\"\"\"\n",
    "    },\n",
    "    \"podcast_intervista\": {\n",
    "        \"description\": \"Podcast informale, intervista giornalistica, conversazione divulgativa\",\n",
    "        \"system_prompt\": \"Sei un redattore editoriale esperto nella sintesi di contenuti divulgativi.\",\n",
    "        \"chunk_prompt\": \"\"\"\n",
    "Riassumi questa parte di podcast/intervista in modo chiaro e accessibile.\n",
    "Metti in evidenza: argomenti trattati, aneddoti rilevanti, informazioni principali condivise.\n",
    "\n",
    "{chunk}\n",
    "\"\"\",\n",
    "        \"final_prompt\": \"\"\"\n",
    "Integra i seguenti estratti in un unico regesto del podcast/intervista.\n",
    "Struttura:\n",
    "- Argomento principale e contesto\n",
    "- Temi sviluppati nel corso della conversazione\n",
    "- Informazioni, storie o esempi notevoli\n",
    "- Messaggi o takeaway finali\n",
    "Evita ripetizioni. Nessun elenco puntato. Massimo 300 parole.\n",
    "\n",
    "{combined}\n",
    "\"\"\"\n",
    "    },\n",
    "    \"musica\": {\n",
    "        \"description\": \"Contenuto musicale, concerto, performance, audio con prevalenza musicale\",\n",
    "        \"system_prompt\": \"Sei un critico musicale esperto nella descrizione di performance e opere musicali.\",\n",
    "        \"chunk_prompt\": \"\"\"\n",
    "Descrivi questa sezione dell'audio musicale.\n",
    "Identifica: genere, atmosfera, struttura percepibile, elementi vocali o strumentali rilevanti.\n",
    "\n",
    "{chunk}\n",
    "\"\"\",\n",
    "        \"final_prompt\": \"\"\"\n",
    "Integra le seguenti descrizioni in un unico regesto della performance/opera musicale.\n",
    "Struttura:\n",
    "- Genere e carattere generale\n",
    "- Struttura e fasi della performance\n",
    "- Elementi distintivi (voci, strumenti, testi se presenti)\n",
    "- Valutazione complessiva dell'opera\n",
    "Evita ripetizioni. Nessun elenco puntato. Massimo 300 parole.\n",
    "\n",
    "{combined}\n",
    "\"\"\"\n",
    "    },\n",
    "    \"altro\": {\n",
    "        \"description\": \"Contenuto non classificabile nelle categorie precedenti\",\n",
    "        \"system_prompt\": \"Sei un redattore esperto nella sintesi di contenuti audio.\",\n",
    "        \"chunk_prompt\": \"\"\"\n",
    "Redigi una sintesi analitica della seguente parte di audio.\n",
    "Linguaggio formale. Evidenzia i contenuti principali espressi.\n",
    "\n",
    "{chunk}\n",
    "\"\"\",\n",
    "        \"final_prompt\": \"\"\"\n",
    "Integra i seguenti estratti in un unico regesto coerente.\n",
    "Evita ripetizioni. Nessun elenco puntato. Massimo 300 parole.\n",
    "\n",
    "{combined}\n",
    "\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def classify_content(text_sample: str, client: anthropic.Anthropic, model: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Classifica il tipo di contenuto audio analizzando un campione di testo.\n",
    "    Restituisce (categoria, motivazione).\n",
    "    \"\"\"\n",
    "    categories_desc = \"\\n\".join([\n",
    "        f\"- '{k}': {v['description']}\"\n",
    "        for k, v in CONTENT_TYPES.items()\n",
    "    ])\n",
    "\n",
    "    # Usa solo i primi 3000 parole come campione per la classificazione\n",
    "    sample = \" \".join(text_sample.split()[:3000])\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=300,\n",
    "        temperature=0,\n",
    "        system=\"Sei un classificatore di contenuti audio. Rispondi SOLO in formato JSON.\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "Analizza il seguente testo trascritto da un audio e classificalo in una delle categorie seguenti:\n",
    "\n",
    "{categories_desc}\n",
    "\n",
    "Testo:\n",
    "{sample}\n",
    "\n",
    "Rispondi ESCLUSIVAMENTE con questo JSON (nessun testo aggiuntivo):\n",
    "{{\n",
    "  \"categoria\": \"<nome_categoria>\",\n",
    "  \"confidenza\": <valore_da_0_a_1>,\n",
    "  \"motivazione\": \"<breve spiegazione>\"\n",
    "}}\n",
    "\"\"\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    import re\n",
    "    raw = response.content[0].text.strip()\n",
    "\n",
    "    # Estrai JSON anche se c'√® testo attorno\n",
    "    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
    "    if match:\n",
    "        result = json.loads(match.group())\n",
    "    else:\n",
    "        result = {\"categoria\": \"altro\", \"confidenza\": 0.0, \"motivazione\": \"Parsing fallito\"}\n",
    "\n",
    "    categoria = result.get(\"categoria\", \"altro\")\n",
    "    if categoria not in CONTENT_TYPES:\n",
    "        categoria = \"altro\"\n",
    "\n",
    "    motivazione = result.get(\"motivazione\", \"\")\n",
    "    confidenza = result.get(\"confidenza\", 0.0)\n",
    "\n",
    "    print(f\"üìÇ Contenuto classificato come: '{categoria}' (confidenza: {confidenza:.0%})\")\n",
    "    print(f\"   Motivazione: {motivazione}\")\n",
    "\n",
    "    return categoria, motivazione\n",
    "\n",
    "\n",
    "# Esegui classificazione\n",
    "print(\"Classifying content type...\")\n",
    "content_category, classification_reason = classify_content(full_text, claude_client, CLAUDE_MODEL)\n",
    "template = CONTENT_TYPES[content_category]\n",
    "\n",
    "# =========================\n",
    "# 9. CHUNKING\n",
    "# =========================\n",
    "\n",
    "def split_chunks(text, limit):\n",
    "    words = text.split()\n",
    "    return [\n",
    "        \" \".join(words[i:i+limit])\n",
    "        for i in range(0, len(words), limit)\n",
    "    ]\n",
    "\n",
    "chunks = split_chunks(full_text, CHUNK_WORD_LIMIT)\n",
    "\n",
    "# =========================\n",
    "# 10. PARTIAL REGESTI WITH CLAUDE (template-based)\n",
    "# =========================\n",
    "\n",
    "partial_regesti = []\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {idx+1}/{len(chunks)}\")\n",
    "\n",
    "    response = claude_client.messages.create(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.2,\n",
    "        system=template[\"system_prompt\"],\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": template[\"chunk_prompt\"].format(chunk=chunk)\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    partial_text = response.content[0].text\n",
    "    partial_regesti.append(partial_text)\n",
    "\n",
    "# =========================\n",
    "# 11. FINAL REGESTO (template-based)\n",
    "# =========================\n",
    "\n",
    "combined = \"\\n\\n\".join(partial_regesti)\n",
    "\n",
    "final_response = claude_client.messages.create(\n",
    "    model=CLAUDE_MODEL,\n",
    "    max_tokens=4000,\n",
    "    temperature=0.2,\n",
    "    system=template[\"system_prompt\"],\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": template[\"final_prompt\"].format(combined=combined)\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "final_regesto = final_response.content[0].text\n",
    "\n",
    "# =========================\n",
    "# 12. SAVE REGESTO OUTPUT\n",
    "# =========================\n",
    "\n",
    "with open(f\"{BASE_NAME}regesto_finale.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_regesto)\n",
    "\n",
    "with open(f\"{BASE_NAME}regesto_finale.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"file_name\": AUDIO_PATH,\n",
    "        \"regesto_finale\": final_regesto\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# =========================\n",
    "# 13. SUMMARY\n",
    "# =========================\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline completa.\")\n",
    "print(f\"   üìÑ trascrizione_diarizzata.txt  ‚Äî trascrizione con speaker e timestamp\")\n",
    "print(f\"   üìã trascrizione_diarizzata.json ‚Äî trascrizione strutturata in JSON\")\n",
    "print(f\"   üìù regesto_finale.txt           ‚Äî regesto accademico\")\n",
    "print(f\"   üì¶ regesto_finale.json          ‚Äî regesto in JSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digimab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
